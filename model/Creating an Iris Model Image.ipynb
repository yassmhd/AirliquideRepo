{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an Iris classifier Docker Image\n",
    "Now it's time to extend the abstract image we just created for Scikit Learn algorithms and implement a Concrete Docker Image with our algorithms/models.\n",
    "\n",
    "![Docker Diagram](../../imgs/DockerScikit_B.jpg)\n",
    "\n",
    "Here, we'll prepare a Docker image with two different algorithms (Using the public iris dataset):\n",
    "1. Logistic regression\n",
    "2. Random Forest Tree\n",
    "\n",
    "We'll use a Sagemaker feature called \"CustomAttributes\" for preparing a dispatcher mechanism. The algorithm we want to use inside our container will be dispatched by this feature.\n",
    "\n",
    "## First, lets create a Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Dockerfile\n",
    "FROM base-image:latest\n",
    "\n",
    "COPY model.py /opt/program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Then, let's create a code that uses scikit-learn as the ML Lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile model.py\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "prefix = '/opt/ml'\n",
    "\n",
    "\n",
    "input_path = os.path.join(prefix, 'input/data')\n",
    "# If something bad happens, write a failure file with the error messages and store here\n",
    "output_path = os.path.join(prefix, 'output')\n",
    "# Everything you store here will be packed into a .tar.gz by Sagemaker and store into S3\n",
    "model_path = os.path.join(prefix, 'model')\n",
    "# This is the hyperparameters you will send to your algorithms through the Estimator\n",
    "param_path = os.path.join(prefix, 'input/config/hyperparameters.json')\n",
    "\n",
    "model_cache = {}\n",
    "\n",
    "def train():\n",
    "    print(\"Training mode\")\n",
    "    \n",
    "    try:\n",
    "        # This algorithm has a single channel of input data called 'training'. Since we run in\n",
    "        # File mode, the input files are copied to the directory specified here.\n",
    "        channel_name='training'\n",
    "        training_path = os.path.join(input_path, channel_name)\n",
    "\n",
    "        hyper_logistic = {}\n",
    "        hyper_random_forest = {}\n",
    "        # Read in any hyperparameters that the user passed with the training job\n",
    "        with open(param_path, 'r') as tc:\n",
    "            is_float = re.compile(r'^\\d+(?:\\.\\d+)$')\n",
    "            is_integer = re.compile(r'^\\d+$')\n",
    "            for key,value in json.load(tc).items():\n",
    "                # workaround to convert numbers from string\n",
    "                if is_float.match(value) is not None:\n",
    "                    value = float(value)\n",
    "                elif is_integer.match(value) is not None:\n",
    "                    value = int(value)\n",
    "                \n",
    "                if key.startswith('logistic'):\n",
    "                    key = key.replace('logistic_', '')\n",
    "                    hyper_logistic[key] = value\n",
    "                if key.startswith('random_forest'):\n",
    "                    key = key.replace('random_forest_', '')\n",
    "                    hyper_random_forest[key] = value\n",
    "\n",
    "        # Take the set of files and read them all into a single pandas dataframe\n",
    "        input_files = [ os.path.join(training_path, file) for file in os.listdir(training_path) ]\n",
    "        if len(input_files) == 0:\n",
    "            raise ValueError(('There are no files in {}.\\\\n' +\n",
    "                              'This usually indicates that the channel ({}) was incorrectly specified,\\\\n' +\n",
    "                              'the data specification in S3 was incorrectly specified or the role specified\\\\n' +\n",
    "                              'does not have permission to access the data.').format(training_path, channel_name))\n",
    "        raw_data = [ pd.read_csv(file, sep=',', header=None ) for file in input_files ]\n",
    "        train_data = pd.concat(raw_data)\n",
    "        \n",
    "        # labels are in the first column\n",
    "        Y = train_data.ix[:,0]\n",
    "        X = train_data.ix[:,1:]\n",
    "        \n",
    "        X_train, X_test, Y_train, Y_test = model_selection.train_test_split(X, Y, test_size=0.33, random_state=7)\n",
    "\n",
    "        algo = \"logistic\"\n",
    "        print(\"Training: %s\" % algo)\n",
    "        model = LogisticRegression()\n",
    "        model.set_params(**hyper_logistic)\n",
    "        model.fit(X_train, Y_train)\n",
    "        print(\"{}: {}\".format( algo, model.score(X_test, Y_test)) )\n",
    "        joblib.dump(model, open(os.path.join(model_path, '%s_model.pkl' % algo), 'wb'))\n",
    "\n",
    "        algo = \"random_forest\"\n",
    "        print(\"Training: %s\" % algo)\n",
    "        model = RandomForestClassifier()\n",
    "        model.set_params(**hyper_random_forest)\n",
    "        model.fit(X_train, Y_train)\n",
    "        print(\"{}: {}\".format( algo, model.score(X_test, Y_test)) )\n",
    "        joblib.dump(model, open(os.path.join(model_path, '%s_model.pkl' % algo), 'wb'))\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Write out an error file. This will be returned as the failureReason in the\n",
    "        # DescribeTrainingJob result.\n",
    "        trc = traceback.format_exc()\n",
    "        with open(os.path.join(output_path, 'failure'), 'w') as s:\n",
    "            s.write('Exception during training: ' + str(e) + '\\\\n' + trc)\n",
    "            \n",
    "        # Printing this causes the exception to be in the training job logs, as well.\n",
    "        print('Exception during training: ' + str(e) + '\\\\n' + trc, file=sys.stderr)\n",
    "        \n",
    "        # A non-zero exit code causes the training job to be marked as Failed.\n",
    "        sys.exit(255)\n",
    "\n",
    "def predict(payload, algo):\n",
    "    if algo is None or payload is None:\n",
    "        raise ValueError( \"You need to inform the algorithm and the payload\" )\n",
    "    \n",
    "    if model_cache.get(algo) is None:\n",
    "        model_filename = os.path.join(model_path, '%s_model.pkl' % algo)\n",
    "        model_cache[algo] = joblib.load(open(model_filename, 'rb'))\n",
    "    \n",
    "    return {\"iris_id\": model_cache[algo].predict( payload ).tolist() }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Finally, let's create the buildspec\n",
    "This file will be used by CodeBuild for creating our base image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile buildspec.yml\n",
    "version: 0.2\n",
    "\n",
    "phases:\n",
    "  install:\n",
    "    runtime-versions:\n",
    "      docker: 18\n",
    "\n",
    "  pre_build:\n",
    "    commands:\n",
    "      - echo Logging in to Amazon ECR...\n",
    "      - $(aws ecr get-login --no-include-email --region $AWS_DEFAULT_REGION)\n",
    "      - docker pull $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/scikit-base:latest\n",
    "      - docker tag $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/scikit-base:latest scikit-base:latest\n",
    "  build:\n",
    "    commands:\n",
    "      - echo Build started on `date`\n",
    "      - echo Building the Docker image...\n",
    "      - docker build -t $IMAGE_REPO_NAME:$IMAGE_TAG .\n",
    "      - docker tag $IMAGE_REPO_NAME:$IMAGE_TAG $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/$IMAGE_REPO_NAME:$IMAGE_TAG\n",
    "\n",
    "  post_build:\n",
    "    commands:\n",
    "      - echo Build completed on `date`\n",
    "      - echo Pushing the Docker image...\n",
    "      - echo docker push $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/$IMAGE_REPO_NAME:$IMAGE_TAG\n",
    "      - docker push $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/$IMAGE_REPO_NAME:$IMAGE_TAG\n",
    "      - echo $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/$IMAGE_REPO_NAME:$IMAGE_TAG > image.url\n",
    "      - echo Done\n",
    "artifacts:\n",
    "  files:\n",
    "    - image.url\n",
    "  name: image_url\n",
    "  discard-paths: yes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the image locally, first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!docker build -f Dockerfile -t model:1.0 ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's do some tests, locally\n",
    "## First, let's define some hyperparameters for both algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"logistic_max_iter\": 100,\n",
    "    \"logistic_solver\": \"lbfgs\",\n",
    "\n",
    "    \"random_forest_max_depth\": 10,\n",
    "    \"random_forest_n_jobs\": 5,\n",
    "    \"random_forest_verbose\": 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "!mkdir -p input/config\n",
    "\n",
    "hyperparameters = dict({key: str(values) for key, values in hyperparameters.items()})\n",
    "with open('input/config/hyperparameters.json', 'w') as f:\n",
    "    f.write(json.dumps(hyperparameters))\n",
    "    f.flush()\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Then, let's prepare a dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p input/data/training\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "dataset = np.insert(iris.data, 0, iris.target,axis=1)\n",
    "\n",
    "pd = pd.DataFrame(data=dataset, columns=['iris_id'] + iris.feature_names)\n",
    "pd.to_csv('input/data/training/iris.csv', header=None, index=False, sep=',', encoding='utf-8')\n",
    "\n",
    "pd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Then, let's test the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p model\n",
    "!rm -f model/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print( \"Training ...\")\n",
    "!docker run --rm --name 'my_model' \\\n",
    "    -v \"$PWD/model:/opt/ml/model\" \\\n",
    "    -v \"$PWD/input:/opt/ml/input\" model:1.0 train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, a basic test with a direct call to our container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print( \"Testing with logistic\")\n",
    "!docker run --rm --name 'my_model' \\\n",
    "    -v \"$PWD/model:/opt/ml/model\" \\\n",
    "    -v \"$PWD/input:/opt/ml/input\" model:1.0 test logistic \"[[4.6, 3.1, 1.5, 0.2]]\"\n",
    "        \n",
    "print( \"Testing with random_forest\")\n",
    "!docker run --rm --name 'my_model' \\\n",
    "    -v \"$PWD/model:/opt/ml/model\" \\\n",
    "    -v \"$PWD/input:/opt/ml/input\" model:1.0 test random_forest \"[[4.6, 3.1, 1.5, 0.2]]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is the serving test. It simulates an Endpoint exposed by Sagemaker\n",
    "\n",
    "After you execute the next cell, this Jupyter notebook will freeze. A webservice will be exposed at the port 8080. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run --rm --name 'my_model' \\\n",
    "    -p 8080:8080 \\\n",
    "    -v \"$PWD/model:/opt/ml/model\" \\\n",
    "    -v \"$PWD/input:/opt/ml/input\" model:1.0 serve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> While the above cell is running, click here [TEST NOTEBOOK](02_Testing%20our%20local%20model%20server.ipynb) to run some tests.\n",
    "\n",
    "> After you finish the tests, press **STOP**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before we push our code to the repo, let's check the building process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "sts_client = boto3.client(\"sts\")\n",
    "session = boto3.session.Session()\n",
    "\n",
    "account_id = sts_client.get_caller_identity()[\"Account\"]\n",
    "region = session.region_name\n",
    "credentials = session.get_credentials()\n",
    "credentials = credentials.get_frozen_credentials()\n",
    "\n",
    "repo_name='model'\n",
    "image_tag='test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p tests\n",
    "!cp model.py Dockerfile buildspec.yml tests/\n",
    "with open('tests/vars.env', 'w') as f:\n",
    "    f.write(\"AWS_ACCOUNT_ID=%s\\n\" % account_id)\n",
    "    f.write(\"IMAGE_TAG=%s\\n\" % image_tag)\n",
    "    f.write(\"IMAGE_REPO_NAME=%s\\n\" % repo_name)\n",
    "    f.write(\"AWS_DEFAULT_REGION=%s\\n\" % region)\n",
    "    f.write(\"AWS_ACCESS_KEY_ID=%s\\n\" % credentials.access_key)\n",
    "    f.write(\"AWS_SECRET_ACCESS_KEY=%s\\n\" % credentials.secret_key)\n",
    "    f.write(\"AWS_SESSION_TOKEN=%s\\n\" % credentials.token )\n",
    "    f.close()\n",
    "\n",
    "!cat tests/vars.env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "!/tmp/aws-codebuild/local_builds/codebuild_build.sh \\\n",
    "    -a \"$PWD/tests/output\" \\\n",
    "    -s \"$PWD/tests\" \\\n",
    "    -i \"samirsouza/aws-codebuild-standard:2.0\" \\\n",
    "    -e \"$PWD/tests/vars.env\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ok, now it's time to push everything to the repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cd ../docker\n",
    "cp $OLDPWD/buildspec.yml $OLDPWD/model.py $OLDPWD/Dockerfile .\n",
    "\n",
    "git add --all\n",
    "git commit -a -m \" - files for building an iris model image\"\n",
    "git push"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ok, now open the AWS console in another tab and go to the CodePipeline console to see the status of our building pipeline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}